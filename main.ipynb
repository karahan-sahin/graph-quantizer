{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install opencv-python\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vector_quantize_pytorch import VectorQuantize, ResidualVQ\n",
    "\n",
    "from lib.config import *\n",
    "from lib.encoder.vqvae import VQVAE\n",
    "from lib.utils.pose import get_pose_estimation\n",
    "from lib.encoder.ffn import FFNEncoder, FFNDecoder\n",
    "from lib.encoder.cnn import CNNEncoder, CNNDecoder\n",
    "from lib.data.dataset import PoseDataset, PoseDistanceDataset\n",
    "from lib.train.run_autoencoder_training import AutoencoderTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from lib.encoder.quantizer import Quantizer\n",
    "\n",
    "quantizer = Quantizer(\n",
    "    base_model='experiments/encoder.pt',\n",
    "    num_frames=25,\n",
    "    stride=1,\n",
    "    num_codebooks=5,\n",
    "    codebook_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array = quantizer.process_video('dataset/corpus/ABARTMAK_0.mp4')\n",
    "quantized, indices = quantizer.quantize(video_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pose Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 'dataset/corpus/ABARTMAK_0.mp4'\n",
    "SAMPLE_POSE = get_pose_estimation(SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_pose_array(SAMPLE_POSE):\n",
    "    \"\"\"Converts the pose data into a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    POSE_RAW = pd.DataFrame(SAMPLE_POSE['pose'])\n",
    "    RIGHT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['right'])\n",
    "    LEFT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['left'])\n",
    "\n",
    "    POSE_DF = {}\n",
    "\n",
    "    for col in POSE_RAW.columns:\n",
    "        POSE_DF[ 'POSE_' + col + '_X'] = POSE_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'POSE_' + col + '_Y'] = POSE_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'POSE_' + col + '_Z'] = POSE_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF[col + '_viz'] = POSE_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    for col in RIGHT_HAND_RAW.columns:\n",
    "        POSE_DF[ 'RIGHT_' + col + '_X' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'RIGHT_' + col + '_Y' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'RIGHT_' + col + '_Z' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF['RIGHT_' + col + '_viz'] = RIGHT_HAND_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    for col in LEFT_HAND_RAW.columns:\n",
    "        POSE_DF[ 'LEFT_' + col + '_X' ] = LEFT_HAND_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'LEFT_' + col + '_Y' ] = LEFT_HAND_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'LEFT_' + col + '_Z' ] = LEFT_HAND_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF['LEFT_' + col + '_viz'] = LEFT_HAND_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    POSE_DF = pd.DataFrame(POSE_DF)\n",
    "\n",
    "    return POSE_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_DF = get_pose_array(SAMPLE_POSE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_DF = POSE_DF.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(POSE_DF):\n",
    "    \"\"\"Converts the pose data into a numpy array of distance matrices\n",
    "    \"\"\"\n",
    "    x_cols = [col for col in POSE_DF.columns if col.endswith('_X')]\n",
    "    y_cols = [col for col in POSE_DF.columns if col.endswith('_Y')]\n",
    "    z_cols = [col for col in POSE_DF.columns if col.endswith('_Z')]\n",
    "\n",
    "    frames = []\n",
    "    for i in range(1, POSE_DF.shape[0]):\n",
    "        x_row = POSE_DF[x_cols].iloc[i].to_numpy()\n",
    "        y_row = POSE_DF[y_cols].iloc[i].to_numpy()\n",
    "        z_row = POSE_DF[z_cols].iloc[i].to_numpy()\n",
    "\n",
    "        def get_difference_matrix(row):\n",
    "            m, n = np.meshgrid(row, row)\n",
    "            out = m-n\n",
    "            return out\n",
    "\n",
    "        x_diff = get_difference_matrix(x_row)\n",
    "        y_diff = get_difference_matrix(y_row)\n",
    "        z_diff = get_difference_matrix(z_row)\n",
    "\n",
    "        frame = np.stack([x_diff, y_diff, z_diff], axis=2)\n",
    "        frames.append(frame)\n",
    "\n",
    "    frames = np.stack(frames, axis=0)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'dataset/adjacency'\n",
    "POSE_PATH = 'dataset/pose'\n",
    "\n",
    "for file in tqdm(glob.glob('dataset/pose/*.npy')):\n",
    "    if os.path.exists(os.path.join(OUT_PATH, os.path.basename(file).replace('.mp4', '.npy'))):\n",
    "        # print('Skipping', file)\n",
    "        continue\n",
    "    with open(file, 'rb') as f:\n",
    "        array = np.load(f, allow_pickle=True)\n",
    "        # replace nan with 0 \n",
    "        array = np.nan_to_num(array)\n",
    "    pose_df = pd.DataFrame(array, columns=POSE_DF.columns)\n",
    "    pose_df = pose_df.replace(np.nan,0)\n",
    "    MATRICES = get_matrices(pose_df)\n",
    "    # print(MATRICES.shape)\n",
    "    np.save(os.path.join(OUT_PATH, os.path.basename(file).replace('.mp4', '.npy')), MATRICES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(x_diff, columns=[col for col in POSE_DF.columns if col.endswith('_X')], index=[col for col in POSE_DF.columns if col.endswith('_X')]).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    ARRAY_PATH = 'dataset/pose/'\n",
    "    for datapath in tqdm(glob.glob('dataset/corpus/*.mp4')):\n",
    "        print(datapath)\n",
    "        pose = get_pose_estimation(datapath)\n",
    "        pose_array = get_pose_array(pose)\n",
    "        print(pose_array.shape, datapath)\n",
    "        dname = datapath.split('/')[-1].replace('.mp4', '.npy')\n",
    "        with open(ARRAY_PATH+'/'+dname, 'wb') as f:\n",
    "            np.save(f, pose_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph Autoencoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/pose/'\n",
    "data = glob.glob(DATA_PATH + '*.npy')\n",
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(X_train)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [00:01<00:00, 534.16it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = PoseDataset(X_val)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kara-nlp/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_ENCODER = FFNEncoder(\n",
    "    input_dim=GLOBAL_CONFIG.MODEL_ENCODER_INPUT_DIM,\n",
    "    hidden_dim=GLOBAL_CONFIG.MODEL_ENCODER_HIDDEN_DIM,\n",
    "    output_dim=GLOBAL_CONFIG.MODEL_ENCODER_OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "MODEL_DECODER = FFNDecoder(\n",
    "    input_dim=GLOBAL_CONFIG.MODEL_DECODER_INPUT_DIM,\n",
    "    hidden_dim=GLOBAL_CONFIG.MODEL_DECODER_HIDDEN_DIM,\n",
    "    output_dim=GLOBAL_CONFIG.MODEL_ENCODER_INPUT_DIM,\n",
    ")\n",
    "\n",
    "MODEL_QUANT = ResidualVQ(\n",
    "    dim = GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    stochastic_sample_codes=True,\n",
    "    num_quantizers=1,      # specify number of quantizers\n",
    "    codebook_size=GLOBAL_CONFIG.MODEL_VQ_NUM_EMBS,    # codebook size           \n",
    "    kmeans_init=True,   # set to True\n",
    "    kmeans_iters=100     # number of kmeans iterations to calculate the centroids for the codebook on init\n",
    ")\n",
    "\n",
    "MODEL_VQVAE = VQVAE(\n",
    "    encoder=MODEL_ENCODER,\n",
    "    decoder=MODEL_DECODER,\n",
    "    vq=MODEL_QUANT,\n",
    ")\n",
    "\n",
    "trainer = AutoencoderTrainer(\n",
    "    model=MODEL_VQVAE,\n",
    "    learning_rate=GLOBAL_CONFIG.LEARNING_RATE,\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=GLOBAL_CONFIG.NUM_EPOCHS,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_VQVAE.eval()\n",
    "\n",
    "dfs = []\n",
    "for train_sample in tqdm(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        quantized, indices, commitment_loss = MODEL_VQVAE(train_sample['array'].float())\n",
    "        dfs.append(pd.DataFrame({\n",
    "            'videos': train_sample['token'],\n",
    "            'labels': indices.detach().cpu().numpy().reshape(-1),\n",
    "            'frame': train_sample['frame'].detach().cpu().numpy().reshape(-1)\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. 3D-CNN Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/adjacency/'\n",
    "data = glob.glob(DATA_PATH + '*.npy')[:10]\n",
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 84.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PoseDistanceDataset(X_train)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=PoseDistanceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 53.07it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = PoseDistanceDataset(X_val)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=PoseDistanceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_ENCODER = CNNEncoder(\n",
    "    input_channels=3,\n",
    ")\n",
    "\n",
    "MODEL_DECODER = CNNDecoder(\n",
    "    output_channels=3,\n",
    ")\n",
    "\n",
    "MODEL_QUANT = ResidualVQ(\n",
    "    dim = GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    stochastic_sample_codes=True,\n",
    "    num_quantizers=1,      # specify number of quantizers\n",
    "    codebook_size=GLOBAL_CONFIG.MODEL_VQ_NUM_EMBS,    # codebook size           \n",
    "    kmeans_init=True,   # set to True\n",
    "    kmeans_iters=10     # number of kmeans iterations to calculate the centroids for the codebook on init\n",
    ")\n",
    "\n",
    "MODEL_VQVAE = VQVAE(\n",
    "    encoder=MODEL_ENCODER,\n",
    "    decoder=MODEL_DECODER,\n",
    "    vq=MODEL_QUANT,\n",
    ")\n",
    "\n",
    "trainer = AutoencoderTrainer(\n",
    "    model=MODEL_VQVAE,\n",
    "    learning_rate=GLOBAL_CONFIG.LEARNING_RATE,\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=GLOBAL_CONFIG.NUM_EPOCHS,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_VQVAE.eval()\n",
    "\n",
    "dfs = []\n",
    "for train_sample in tqdm(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        quantized, indices, commitment_loss = MODEL_VQVAE(train_sample['array'].float())\n",
    "        # print(indices)\n",
    "        # print(indices.shape)\n",
    "        # print(indices.detach().cpu().numpy().reshape(-1))\n",
    "        dfs.append(pd.DataFrame({\n",
    "            'videos': train_sample['tokens'],\n",
    "            'labels': indices.detach().cpu().numpy().reshape(-1),\n",
    "            'start_idx': train_sample['start_idx'],\n",
    "            'end_idx': train_sample['end_idx']\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videos</th>\n",
       "      <th>labels</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C╠ğIG╠åNEMEK_0</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SADE_1</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C╠ğIG╠åNEMEK_0</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YABANCI_1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EMEK_2</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEG╠åER_1</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SADE_1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C╠ğIG╠åNEMEK_0</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C╠ğIG╠åNEMEK_0</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOMBA_0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            videos  labels  start_idx  end_idx\n",
       "0   C╠ğIG╠åNEMEK_0       6         17       22\n",
       "1           SADE_1       6         38       43\n",
       "2   C╠ğIG╠åNEMEK_0       6         97      102\n",
       "3        YABANCI_1       6         23       28\n",
       "4           EMEK_2       6         13       18\n",
       "..             ...     ...        ...      ...\n",
       "5        DEG╠åER_1       6         36       41\n",
       "6           SADE_1       6          7       12\n",
       "7   C╠ğIG╠åNEMEK_0       6         67       72\n",
       "0   C╠ğIG╠åNEMEK_0       6         62       67\n",
       "1          BOMBA_0       6         13       18\n",
       "\n",
       "[490 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "for rec in tqdm(df[df['labels'] == 6].to_dict(orient='records')):\n",
    "    # save frame video to disk\n",
    "    video = rec['videos'].split('.')[0]\n",
    "    video_path = f\"dataset/corpus/{video}.mp4\"\n",
    "    start_idx = rec['start_idx']\n",
    "    end_idx = rec['start_idx']\n",
    "    label = rec['labels']\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(f'analyze/quantization/{label}'):\n",
    "        os.mkdir(f'analyze/quantization/{label}')\n",
    "    \n",
    "    extract = True\n",
    "    frames = []\n",
    "    i = 0\n",
    "    while extract:\n",
    "        ret, frame = cap.read()\n",
    "        i += 1\n",
    "        if i >= start_idx and i <= end_idx:\n",
    "            frames.append(frame)\n",
    "        \n",
    "    # write video from frames\n",
    "    out = cv2.VideoWriter(f'analyze/quantization/{label}/{video}.avi', cv2.VideoWriter_fourcc(*'DIVX'), 15, (frame.shape[1], frame.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
